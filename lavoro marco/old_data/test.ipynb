{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "CACHE_DIR = Path(\"data/who\")\n",
    "PAGE_SIZE = 1000\n",
    "TIMEOUT = 30\n",
    "\n",
    "INDICATORS: List[str] = [\n",
    "    \"MDG_0000000020\",\"TB_1\",\"TB_e_inc_num\",\n",
    "    \"MALARIA_EST_INCIDENCE\",\"MALARIA_TOTAL_CASES\",\"MALARIA_EST_DEATHS\",\n",
    "    \"MALARIA_ITN_COVERAGE\",\"MALARIA_IRS_COVERAGE\",\"MALARIA_IPTP3_COVERAGE\",\n",
    "    \"HIV_0000000026\",\"SDGHIV\",\"MDG_0000000029\"\n",
    "]\n",
    "\n",
    "def _session_with_retries() -> requests.Session:\n",
    "    from urllib3.util.retry import Retry\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=0.5,\n",
    "                    status_forcelist=(429,500,502,503,504),\n",
    "                    allowed_methods=frozenset([\"GET\"]))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def fetch_indicator_raw(\n",
    "    code: str,\n",
    "    include_regions: bool = False,\n",
    "    year_min: Optional[int] = None,\n",
    "    year_max: Optional[int] = None,\n",
    "    session: Optional[requests.Session] = None\n",
    ") -> list[dict]:\n",
    "    sess = session or _session_with_retries()\n",
    "    rows, skip = [], 0\n",
    "    def _get(params):\n",
    "        r = sess.get(f\"{BASE}/{code}\", params=params, timeout=TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        return js.get(\"value\") if isinstance(js, dict) else js\n",
    "    while True:\n",
    "        params = {\"$format\":\"json\",\"$top\":PAGE_SIZE,\"$skip\":skip}\n",
    "        filt = []\n",
    "        if not include_regions:\n",
    "            filt.append(\"SpatialDimType eq 'COUNTRY'\")\n",
    "        if year_min is not None:\n",
    "            filt.append(f\"TimeDim ge {int(year_min)}\")\n",
    "        if year_max is not None:\n",
    "            filt.append(f\"TimeDim le {int(year_max)}\")\n",
    "        if filt:\n",
    "            params[\"$filter\"] = \" and \".join(filt)\n",
    "        try:\n",
    "            batch = _get(params)\n",
    "        except requests.HTTPError:\n",
    "            params.pop(\"$filter\", None)\n",
    "            try:\n",
    "                batch = _get(params)\n",
    "            except requests.HTTPError:\n",
    "                batch = _get({\"$format\":\"json\",\"$top\":PAGE_SIZE,\"$skip\":skip})\n",
    "        batch = batch or []\n",
    "        rows.extend(batch)\n",
    "        if len(batch) < PAGE_SIZE:\n",
    "            break\n",
    "        skip += PAGE_SIZE\n",
    "    return rows\n",
    "\n",
    "def normalize_df(raw_rows: list[dict], indicator: str) -> pd.DataFrame:\n",
    "    if not raw_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"indicator\",\"iso3\",\"year\",\"value\",\"low\",\"high\",\n",
    "            \"dim1\",\"dim1_type\",\"who_region\",\"who_region_code\",\n",
    "            \"spatial_type\",\"Date\"\n",
    "        ])\n",
    "    df = pd.DataFrame(raw_rows)\n",
    "    rename_map = {\n",
    "        \"SpatialDim\":\"iso3\",\"TimeDim\":\"year\",\"NumericValue\":\"value\",\n",
    "        \"Low\":\"low\",\"High\":\"high\",\"Dim1\":\"dim1\",\"Dim1Type\":\"dim1_type\",\n",
    "        \"ParentLocation\":\"who_region\",\"ParentLocationCode\":\"who_region_code\",\n",
    "        \"SpatialDimType\":\"spatial_type\"\n",
    "    }\n",
    "    df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
    "    if \"value\" not in df.columns or df[\"value\"].isna().all():\n",
    "        if \"Value\" in df.columns:\n",
    "            def parse_numeric(s):\n",
    "                if pd.isna(s): return None\n",
    "                if isinstance(s,(int,float)): return float(s)\n",
    "                m = re.match(r\"^\\s*([+-]?\\d+(?:[\\.,]\\d+)?)\", str(s))\n",
    "                return float(m.group(1).replace(\",\",\".\")) if m else None\n",
    "            df[\"value\"] = df[\"Value\"].map(parse_numeric)\n",
    "    if \"year\" in df: df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    for c in (\"value\",\"low\",\"high\"):\n",
    "        if c in df: df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    for c in (\"iso3\",\"dim1\",\"dim1_type\",\"who_region\",\"who_region_code\"):\n",
    "        if c in df: df[c] = df[c].astype(\"string\")\n",
    "    if \"spatial_type\" in df: df[\"spatial_type\"] = df[\"spatial_type\"].astype(\"category\")\n",
    "    df[\"indicator\"] = indicator\n",
    "    cols = [\"indicator\",\"iso3\",\"year\",\"value\",\"low\",\"high\",\n",
    "            \"dim1\",\"dim1_type\",\"who_region\",\"who_region_code\",\"spatial_type\",\"Date\"]\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    return df\n",
    "\n",
    "def load_or_download_indicator(\n",
    "    code: str,\n",
    "    refresh: bool = False,\n",
    "    include_regions: bool = False,\n",
    "    year_min: Optional[int] = None,\n",
    "    year_max: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    path = CACHE_DIR / f\"{code}.parquet\"\n",
    "    if path.exists() and not refresh:\n",
    "        return pd.read_parquet(path)\n",
    "    raw = fetch_indicator_raw(code, include_regions, year_min, year_max)\n",
    "    df = normalize_df(raw, indicator=code)\n",
    "    if not include_regions:\n",
    "        if \"spatial_type\" in df.columns:\n",
    "            df = df[df[\"spatial_type\"].astype(str) == \"COUNTRY\"]\n",
    "        elif \"iso3\" in df.columns:\n",
    "            df = df[df[\"iso3\"].str.fullmatch(r\"[A-Z]{3}\", na=False)]\n",
    "    df.to_parquet(path, index=False)\n",
    "    return df\n",
    "\n",
    "def download_each(\n",
    "    indicators: List[str] = INDICATORS,\n",
    "    refresh: bool = False,\n",
    "    include_regions: bool = False,\n",
    "    year_min: Optional[int] = None,\n",
    "    year_max: Optional[int] = None,\n",
    "    save: bool = True,\n",
    "    out_dir: str = \"data/who\",\n",
    "    fmt: str = \"parquet\",\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "    for code in indicators:\n",
    "        try:\n",
    "            df = load_or_download_indicator(\n",
    "                code, refresh, include_regions, year_min, year_max\n",
    "            )\n",
    "            dfs[code] = df\n",
    "            if save:\n",
    "                if fmt == \"parquet\":\n",
    "                    df.to_parquet(f\"{out_dir}/{code}.parquet\", index=False)\n",
    "                elif fmt == \"csv\":\n",
    "                    df.to_csv(f\"{out_dir}/{code}.csv\", index=False)\n",
    "                else:\n",
    "                    raise ValueError(\"fmt deve essere 'parquet' o 'csv'\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {code}: {e}\")\n",
    "    return dfs\n",
    "\n",
    "# --- esempio d'uso ---\n",
    "# dfs = download_each(include_regions=False, fmt=\"parquet\")\n",
    "# print(dfs[\"MDG_0000000020\"].head())\n",
    "\n",
    "\n",
    "dfs = download_each(indicators=[\n",
    "    \"MDG_0000000020\",\"TB_1\",\"TB_e_inc_num\",\n",
    "    \"MALARIA_EST_INCIDENCE\",\"MALARIA_TOTAL_CASES\",\"MALARIA_EST_DEATHS\",\n",
    "    \"MALARIA_ITN_COVERAGE\",\"MALARIA_IRS_COVERAGE\",\"MALARIA_IPTP3_COVERAGE\",\n",
    "    \"HIV_0000000026\",\"SDGHIV\",\"MDG_0000000029\"\n",
    "], include_regions=False, fmt=\"csv\")\n",
    "\n",
    "#tb = dfs[\"MDG_0000000020\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
